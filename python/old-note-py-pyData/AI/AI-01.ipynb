{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络中的超参数(hyperparameter)\n",
    "\n",
    "- 学习率: $\\eta$, learning rate,  lr\n",
    "- 批量大小: $|\\mathcal{B}|$, batch size。 表示每个小批量中的样本数\n",
    "- 迭代周期: epochs\n",
    "- 正则化参数\n",
    "  - $\\lambda$, 正则化参数, $L_1$ 或 $L_2$ 正则化强度。 是惩罚项($\\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2$)的系数\n",
    "    > $$L(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2$$\n",
    "  - weight_decay, 这个变量代表 权重衰退(weight decay) 的超参数。 权重衰退又被称为 $L_2$ 正则化。 \n",
    "    > $\\lambda$ 和 weight_decay 有些类似但又不一样。 我的理解是, weight_decay 是权重衰退的超参数的统称; 而 $\\lambda$ 是中惩罚项的系数, 它可以认为是权重衰退的一个超参数。\n",
    "- 神经元数量\n",
    "- 神经元层数\n",
    "- 激活函数。 常见的有 sigmoid、ReLU、tanh\n",
    "- 优化器。 SGD、Adam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`input_dim=2`\n",
    "`input_shape=(None, 2)`\n",
    "`input_shape=(2,)`\n",
    "这三个是等效的。 "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
